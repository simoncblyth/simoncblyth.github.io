<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>PyCUDA Memory &mdash; Env  documentation</title>
    
    <link rel="stylesheet" href="../../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../_static/highstock/highstock.js"></script>
    <script type="text/javascript" src="../../_static/highstock/modules/exporting.js"></script>
    <link rel="top" title="Env  documentation" href="../../" />
    <link rel="up" title="pycuda" href="../" />
    <link rel="next" title="geant4" href="../../geant4/" />
    <link rel="prev" title="NVIDIA Interop Examples" href="../pycuda_pyopengl_interop/interop_nvidia_examples/" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../../geant4/" title="geant4"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../pycuda_pyopengl_interop/interop_nvidia_examples/" title="NVIDIA Interop Examples"
             accesskey="P">previous</a> |</li>
    <li><a href="/tracs/env/timeline">env</a> &raquo;</li>
    
        <li><a href="../../">Env  documentation</a> &raquo;</li>

          <li><a href="../" accesskey="U">pycuda</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script><h3>Links</h3>
<ul class="this-page-menu">
	<li><a href="/tracs/env"> env </a>  <a href="/tracs/env/timeline"> tl </a> <a href="/repos/env/trunk"> repo </a> <a href="/e"> edocs </a> </li>
	<li><a href="/tracs/heprez"> heprez </a>  <a href="/tracs/heprez/timeline"> tl </a> <a href="/repos/heprez/trunk"> repo</a> <a href="/h">hdocs</a>   </li>
        <li><a href="/e/scm/monitor/" > backup status </a> </li>
</ul>

<h3>Content Skeleton</h3>

<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/">Installing <strong>env</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../base/">Base Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log/May2012/">LOG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../TODO/">TODO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sysadmin/">Sys Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plot/">Plotting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../scm/">SCM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trac/">Trac</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../root/">ROOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sphinxext/">Sphinx Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../matplotlib/">Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nose/">nose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../svn/">SVN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../npy/">Numerical Python, numpy et al</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pypy/">PyPy : faster python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tools/">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mysqlhotcopy/">MySQL hotcopy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mysql/">MySQL Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sqlite/">SQLite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../db/">DB scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../qxml/">QXML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fossil/">Fossil</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java/">Java Demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda/">cuda</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../">pycuda</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../pycuda_bash/">PYCUDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pycuda_refs/">PyCUDA References</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pycuda_debugging/">PyCUDA Debugging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pycuda_error_handling/">PyCUDA Error Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pycuda_pyopengl_interop/">pycuda_pyopengl_interop</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">PyCUDA Memory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../geant4/">geant4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../muon_simulation/">muon_simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chroma/">chroma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../llvm/">llvm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../graphics/">Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda/">cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../opencl/">opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linux/">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cloud/">Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../package_management/">Package Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ui/">ui</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debugging/">debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mercurial/">mercurial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../javascript/">javascript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nuwa/">nuwa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ccgpu/">ccgpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pygame/">pygame</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zeromq/">zeromq</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../doc/">doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../osx/">osx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hg/">hg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simoncblyth.bitbucket.org/">simoncblyth.bitbucket.org</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../numpy/">numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../presentation/">Muon Simulation Presentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optix/">optix</a></li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../_sources/pycuda/pycuda_memory.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
  <h4>Previous topic</h4>
  <p class="topless"><a href="../pycuda_pyopengl_interop/interop_nvidia_examples/"
                        title="previous chapter">NVIDIA Interop Examples</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../../geant4/"
                        title="next chapter">geant4</a></p>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="pycuda-memory">
<h1>PyCUDA Memory<a class="headerlink" href="#pycuda-memory" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>device memory, host memory, pinned memory, mapped memory, free-ing memory</li>
</ul>
<div class="section" id="observations">
<h2>Observations<a class="headerlink" href="#observations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="gpu-memory-cleanup-issue">
<h3>GPU Memory Cleanup Issue ?<a class="headerlink" href="#gpu-memory-cleanup-issue" title="Permalink to this headline">¶</a></h3>
<p>Suspect problem with PyCUDA/Chroma GPU memory cleanup, as usually
finding chroma propagation runtimes (observerd with non-vbo variant)
are a factor of 3 less in the morning, at the start of work.</p>
</div>
</div>
<div class="section" id="freeing-device-memory">
<h2>Freeing Device Memory<a class="headerlink" href="#freeing-device-memory" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://lists.tiker.net/pipermail/pycuda/2010-April/002333.html">http://lists.tiker.net/pipermail/pycuda/2010-April/002333.html</a></li>
</ul>
<p>&gt; Can I manually free GPUarray instances?  If not, can I somehow manually
&gt; remove all PyCUDA stuff from memory?</p>
<p>Python deinitialises objects as soon as the reference count for them
becomes zero. If you need to do it explicitly, I think just &#8220;del
gpuarray_obj&#8221; will be enough. At least, it worked for me.</p>
</div>
<div class="section" id="pagelocked-aka-pinned-memory-device-mapped-memory">
<h2>Pagelocked (aka pinned) memory, Device mapped memory<a class="headerlink" href="#pagelocked-aka-pinned-memory-device-mapped-memory" title="Permalink to this headline">¶</a></h2>
<p>Chroma GPUGeometry supposedly uses device mapped memory</p>
<p><cite>chroma/chroma/gpu/geometry.py</cite>:</p>
<div class="highlight-python"><pre>..1 import numpy as np
  2 import pycuda.driver as cuda
  3 from pycuda import gpuarray as ga
  4 from pycuda import characterize
  5
  6 from collections import OrderedDict
  7
  8 from chroma.geometry import standard_wavelengths
  9 from chroma.gpu.tools import get_cu_module, get_cu_source, cuda_options, \
 10     chunk_iterator, format_array, format_size, to_uint3, to_float3, \
 11     make_gpu_struct, GPUFuncs, mapped_empty, Mapped
 12
 13 #from chroma.log import logger
 14 import logging
 15 log = logging.getLogger(__name__)
 16
 17 class GPUGeometry(object):
 18     def __init__(self, geometry, wavelengths=None, print_usage=False, min_free_gpu_mem=300e6):
 19         log.info("GPUGeometry.__init__ min_free_gpu_mem %s ", min_free_gpu_mem)
 20
...
143         self.vertices = mapped_empty(shape=len(geometry.mesh.vertices),
144                                      dtype=ga.vec.float3,
145                                      write_combined=True)
146         self.triangles = mapped_empty(shape=len(geometry.mesh.triangles),
147                                       dtype=ga.vec.uint3,
148                                       write_combined=True)
149         self.vertices[:] = to_float3(geometry.mesh.vertices)
150         self.triangles[:] = to_uint3(geometry.mesh.triangles)
...
202         # See if there is enough memory to put the and/ortriangles back on the GPU
203         gpu_free, gpu_total = cuda.mem_get_info()
204         if self.triangles.nbytes &lt; (gpu_free - min_free_gpu_mem):
205             self.triangles = ga.to_gpu(self.triangles)
206             log.info('Optimization: Sufficient memory to move triangles onto GPU')
207             triangle_gpu = 1
208         else:
209             triangle_gpu = 0
210         pass</pre>
</div>
</div>
<div class="section" id="pagelocked-or-pinned-host-memory">
<h2>pagelocked or pinned host memory<a class="headerlink" href="#pagelocked-or-pinned-host-memory" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://devblogs.nvidia.com/parallelforall/how-optimize-data-transfers-cuda-cc/">http://devblogs.nvidia.com/parallelforall/how-optimize-data-transfers-cuda-cc/</a></li>
</ul>
<p>Host (CPU) data allocations are pageable by default. The GPU cannot access data
directly from pageable host memory, so when a data transfer from pageable host
memory to device memory is invoked, the CUDA driver must first allocate a
temporary page-locked, or “pinned”, host array, copy the host data to the
pinned array, and then transfer the data from the pinned array to device
memory, as illustrated below.</p>
<p>As you can see in the figure, pinned memory is used as a staging area for
transfers from the device to the host. We can avoid the cost of the transfer
between pageable and pinned host arrays by directly allocating our host arrays
in pinned memory. Allocate pinned host memory in CUDA C/C++ using
cudaMallocHost() or cudaHostAlloc(), and deallocate it with cudaFreeHost().</p>
</div>
<div class="section" id="cumemhostalloc-cuda-host-memory-allocation">
<h2>cuMemHostAlloc : CUDA Host Memory Allocation<a class="headerlink" href="#cumemhostalloc-cuda-host-memory-allocation" title="Permalink to this headline">¶</a></h2>
<p>TODO: find the 5.5 version of these docs</p>
<ul class="simple">
<li><a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_1/rel/toolkit/docs/online/group__CUDA__MEM_g572ca4011bfcb25034888a14d4e035b9.html">http://developer.download.nvidia.com/compute/cuda/4_1/rel/toolkit/docs/online/group__CUDA__MEM_g572ca4011bfcb25034888a14d4e035b9.html</a></li>
</ul>
<div class="highlight-python"><pre>CUresult cuMemHostAlloc (void ** pp, size_t bytesize, unsigned int Flags)</pre>
</div>
<p>The Flags parameter enables different options to be specified that affect the allocation, as follows.</p>
<dl class="docutils">
<dt>CU_MEMHOSTALLOC_PORTABLE</dt>
<dd>The memory returned by this call will be considered as pinned memory by all CUDA contexts,
not just the one that performed the allocation.</dd>
<dt>CU_MEMHOSTALLOC_DEVICEMAP</dt>
<dd>Maps the allocation into the CUDA address space.
The device pointer to the memory may be obtained by calling cuMemHostGetDevicePointer().
This feature is available only on GPUs with compute capability greater than or equal to 1.1.</dd>
<dt>CU_MEMHOSTALLOC_WRITECOMBINED</dt>
<dd>Allocates the memory as write-combined (WC).
WC memory can be transferred across the PCI Express bus more quickly on some system configurations,
but cannot be read efficiently by most CPUs.
WC memory is a good option for buffers that will be written by the CPU and read by the GPU
via mapped pinned memory or host-&gt;device transfers.</dd>
</dl>
<p>All of these flags are orthogonal to one another: a developer may allocate memory
that is portable, mapped and/or write-combined with no restrictions.</p>
<p><strong>The CUDA context must have been created with the CU_CTX_MAP_HOST flag
in order for the CU_MEMHOSTALLOC_MAPPED flag to have any effect.</strong></p>
<blockquote>
<div><ul class="simple">
<li>is that a typo? CU_MEMHOSTALLOC_MAPPED should be CU_MEMHOSTALLOC_DEVICEMAP</li>
<li>is Chroma/PyCUDA context being created with the requisite flag ?</li>
</ul>
</div></blockquote>
<p>The CU_MEMHOSTALLOC_MAPPED flag may be specified on CUDA contexts for devices
that do not support mapped pinned memory. The failure is deferred to
cuMemHostGetDevicePointer() because the memory may be mapped into other CUDA
contexts via the CU_MEMHOSTALLOC_PORTABLE flag.</p>
<p>The memory allocated by this function must be freed with cuMemFreeHost().</p>
</div>
<div class="section" id="cuctxcreate">
<h2>cuCtxCreate<a class="headerlink" href="#cuctxcreate" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_1/rel/toolkit/docs/online/group__CUDA__CTX_g65dc0012348bc84810e2103a40d8e2cf.html">http://developer.download.nvidia.com/compute/cuda/4_1/rel/toolkit/docs/online/group__CUDA__CTX_g65dc0012348bc84810e2103a40d8e2cf.html</a></li>
</ul>
<div class="highlight-python"><pre>CUresult cuCtxCreate    (   CUcontext *     pctx, unsigned int    flags, CUdevice    dev  )</pre>
</div>
<dl class="docutils">
<dt>CU_CTX_MAP_HOST</dt>
<dd>Instruct CUDA to support mapped pinned allocations. This flag must be set in order
to allocate pinned host memory that is accessible to the GPU.</dd>
</dl>
</div>
<div class="section" id="cu-ctx-map-host">
<h2>CU_CTX_MAP_HOST<a class="headerlink" href="#cu-ctx-map-host" title="Permalink to this headline">¶</a></h2>
<p><cite>/usr/local/env/chroma_env/build/build_pycuda/pycuda/src/wrapper/wrap_cudadrv.cpp</cite>:</p>
<div class="highlight-python"><pre>525
526 #if CUDAPP_CUDA_VERSION &gt;= 2000
527   py::enum_&lt;CUctx_flags&gt;("ctx_flags")
528     .value("SCHED_AUTO", CU_CTX_SCHED_AUTO)
529     .value("SCHED_SPIN", CU_CTX_SCHED_SPIN)
530     .value("SCHED_YIELD", CU_CTX_SCHED_YIELD)
531     .value("SCHED_MASK", CU_CTX_SCHED_MASK)
532 #if CUDAPP_CUDA_VERSION &gt;= 2020 &amp;&amp; CUDAPP_CUDA_VERSION &lt; 4000
533     .value("BLOCKING_SYNC", CU_CTX_BLOCKING_SYNC)
534     .value("SCHED_BLOCKING_SYNC", CU_CTX_BLOCKING_SYNC)
535 #endif
536 #if CUDAPP_CUDA_VERSION &gt;= 4000
537     .value("BLOCKING_SYNC", CU_CTX_SCHED_BLOCKING_SYNC)
538     .value("SCHED_BLOCKING_SYNC", CU_CTX_SCHED_BLOCKING_SYNC)
539 #endif
540 #if CUDAPP_CUDA_VERSION &gt;= 2020
541     .value("MAP_HOST", CU_CTX_MAP_HOST)
542 #endif
543 #if CUDAPP_CUDA_VERSION &gt;= 3020
544     .value("LMEM_RESIZE_TO_MAX", CU_CTX_LMEM_RESIZE_TO_MAX)
545 #endif
546     .value("FLAGS_MASK", CU_CTX_FLAGS_MASK)
547     ;
548 #endif</pre>
</div>
<div class="highlight-python"><pre>In [1]: import pycuda.driver as cuda

In [2]: cuda.ctx_flags
Out[2]: pycuda._driver.ctx_flags

In [3]: cuda.ctx_flags.
cuda.ctx_flags.BLOCKING_SYNC        cuda.ctx_flags.conjugate
cuda.ctx_flags.FLAGS_MASK           cuda.ctx_flags.denominator
cuda.ctx_flags.LMEM_RESIZE_TO_MAX   cuda.ctx_flags.imag
cuda.ctx_flags.MAP_HOST             cuda.ctx_flags.mro
cuda.ctx_flags.SCHED_AUTO           cuda.ctx_flags.name
cuda.ctx_flags.SCHED_BLOCKING_SYNC  cuda.ctx_flags.names
cuda.ctx_flags.SCHED_MASK           cuda.ctx_flags.numerator
cuda.ctx_flags.SCHED_SPIN           cuda.ctx_flags.real
cuda.ctx_flags.SCHED_YIELD          cuda.ctx_flags.values
cuda.ctx_flags.bit_length

In [13]: print cuda.ctx_flags.MAP_HOST == 8
True</pre>
</div>
</div>
<div class="section" id="chroma-context-creation">
<h2>chroma context creation<a class="headerlink" href="#chroma-context-creation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><pre>(chroma_env)delta:chroma blyth$ find . -name '*.py' -exec grep -H context {} \;
./benchmark.py:# Generator processes need to fork BEFORE the GPU context is setup
./benchmark.py:    context = gpu.create_cuda_context()
./benchmark.py:    context.pop()
./camera.py:        self.context = gpu.create_cuda_context(self.device_id)
./camera.py:        self.context.pop()
./generator/photon.py:        context = zmq.Context()
./generator/photon.py:        vertex_socket = context.socket(zmq.PULL)
./generator/photon.py:        photon_socket = context.socket(zmq.PUSH)
./generator/photon.py:        self.zmq_context = zmq.Context()
./generator/photon.py:        self.vertex_socket = self.zmq_context.socket(zmq.PUSH)
./generator/photon.py:        self.photon_socket = self.zmq_context.socket(zmq.PULL)
./gpu/tools.py:    Hashability needed for context_dependent_memoize dictates the type
./gpu/tools.py:@pycuda.tools.context_dependent_memoize
./gpu/tools.py:def create_cuda_context(device_id=None):
./gpu/tools.py:    """Initialize and return a CUDA context on the specified device.
./gpu/tools.py:            context = pycuda.tools.make_default_context()
./gpu/tools.py:            context = pycuda.tools.make_default_context()
./gpu/tools.py:        context = device.make_context()
./gpu/tools.py:    context.set_cache_config(cuda.func_cache.PREFER_L1)
./gpu/tools.py:    return context
./loader.py:from chroma.gpu import create_cuda_context
./loader.py:        context = create_cuda_context(cuda_device)
./loader.py:        context.pop()
./sim.py:        self.context = gpu.create_cuda_context(cuda_device)
./sim.py:        self.context.pop()
(chroma_env)delta:chroma blyth$</pre>
</div>
</div>
<div class="section" id="g4daechroma-g4daeview-daechromacontext">
<h2>g4daechroma/g4daeview DAEChromaContext<a class="headerlink" href="#g4daechroma-g4daeview-daechromacontext" title="Permalink to this headline">¶</a></h2>
<p><cite>env/geant4/geometry/collada/g4daeview/daechromacontext.py</cite>:</p>
<div class="highlight-python"><pre>16 import numpy as np
17 import pycuda.gl.autoinit  # after this can use pycuda.gl.BufferObject(unsigned int)
18
19 def pick_seed():
20     """Returns a seed for a random number generator selected using
21     a mixture of the current time and the current process ID."""
22     return int(time.time()) ^ (os.getpid() &lt;&lt; 16)
23
24 class DAEChromaContext(object):</pre>
</div>
<div class="highlight-python"><pre>In [3]: from pycuda.gl import autoinit

In [4]: autoinit??
Type:       module
String Form:&lt;module 'pycuda.gl.autoinit' from '/usr/local/env/chroma_env/lib/python2.7/site-packages/pycuda/gl/autoinit.pyc'&gt;
File:       /usr/local/env/chroma_env/lib/python2.7/site-packages/pycuda/gl/autoinit.py
Source:
import pycuda.driver as cuda
import pycuda.gl as cudagl

cuda.init()
assert cuda.Device.count() &gt;= 1

from pycuda.tools import make_default_context
context = make_default_context(lambda dev: cudagl.make_context(dev))
device = context.get_device()

import atexit
atexit.register(context.pop)</pre>
</div>
</div>
<div class="section" id="pycuda-context-creation">
<h2>pycuda context creation<a class="headerlink" href="#pycuda-context-creation" title="Permalink to this headline">¶</a></h2>
<p><cite>/usr/local/env/chroma_env/build/build_pycuda/pycuda/pycuda/tools.py</cite>:</p>
<div class="highlight-python"><pre>159 def make_default_context(ctx_maker=None):
160     if ctx_maker is None:
161         def ctx_maker(dev):
162             return dev.make_context()
163
164     ndevices = cuda.Device.count()
...
194     # Otherwise, try to use any available device
195     else:
196         for devn in xrange(ndevices):
197             dev = cuda.Device(devn)
198             try:
199                 return ctx_maker(dev)
200             except cuda.Error:
201                 pass</pre>
</div>
<p><cite>/usr/local/env/chroma_env/build/build_pycuda/pycuda/src/wrapper/wrap_cudadrv.cpp</cite>:</p>
<div class="highlight-python"><pre>867   // {{{ device
868   {
869     typedef device cl;
870     py::class_&lt;cl&gt;("Device", py::no_init)
871       .def("__init__", py::make_constructor(make_device))
872 #if CUDAPP_CUDA_VERSION &gt;= 4010
873       .def("__init__", py::make_constructor(make_device_from_pci_bus_id))
874 #endif
875       .DEF_SIMPLE_METHOD(count)
876       .staticmethod("count")
877       .DEF_SIMPLE_METHOD(name)
878 #if CUDAPP_CUDA_VERSION &gt;= 4010
879       .DEF_SIMPLE_METHOD(pci_bus_id)
880 #endif
881       .DEF_SIMPLE_METHOD(compute_capability)
882       .DEF_SIMPLE_METHOD(total_memory)
883       .def("get_attribute", device_get_attribute)
884       .def(py::self == py::self)
885       .def(py::self != py::self)
886       .def("__hash__", &amp;cl::hash)
887       .def("make_context", &amp;cl::make_context,
888           (py::args("self"), py::args("flags")=0))
889 #if CUDAPP_CUDA_VERSION &gt;= 4000
890       .DEF_SIMPLE_METHOD(can_access_peer)
891 #endif
892       ;
893   }</pre>
</div>
<p><cite>/usr/local/env/chroma_env/build/build_pycuda/pycuda/src/cpp/cuda.hpp</cite>:</p>
<div class="highlight-python"><pre>766
767   inline
768   boost::shared_ptr&lt;context&gt; device::make_context(unsigned int flags)
769   {
770     context::prepare_context_switch();
771
772     CUcontext ctx;
773     CUDAPP_CALL_GUARDED(cuCtxCreate, (&amp;ctx, flags, m_device));
774     boost::shared_ptr&lt;context&gt; result(new context(ctx));
775     context_stack::get().push(result);
776     return result;
777   }
778
779</pre>
</div>
</div>
<div class="section" id="mapped-empty">
<h2>mapped_empty<a class="headerlink" href="#mapped-empty" title="Permalink to this headline">¶</a></h2>
<p>The largest Chroma arrays (bvh nodes, vertices, triangles) are
all handled using <strong>mapped_empty</strong>. These use allocator
<strong>pycuda.driver.pagelocked_empty</strong> with mem flags:</p>
<ul class="simple">
<li>pycuda.driver.host_alloc_flags.DEVICEMAP (CU_MEMHOSTALLOC_DEVICEMAP)</li>
<li>pycuda.driver.host_alloc_flags.WRITECOMBINED (CU_MEMHOSTALLOC_WRITECOMBINED)</li>
</ul>
<p><cite>chroma/chroma/gpu/tools.py</cite>:</p>
<div class="highlight-python"><pre>..8 import pycuda.driver as cuda
...
247 def mapped_alloc(pagelocked_alloc_func, shape, dtype, write_combined):
248     '''Returns a pagelocked host array mapped into the CUDA device
249     address space, with a gpudata field set so it just works with CUDA
250     functions.'''
251     flags = cuda.host_alloc_flags.DEVICEMAP
252     if write_combined:
253         flags |= cuda.host_alloc_flags.WRITECOMBINED
254     array = pagelocked_alloc_func(shape=shape, dtype=dtype, mem_flags=flags)
255     return array
256
257 def mapped_empty(shape, dtype, write_combined=False):
258     '''See mapped_alloc()'''
259     return mapped_alloc(cuda.pagelocked_empty, shape, dtype, write_combined)</pre>
</div>
<p><cite>/usr/local/env/chroma_env/build/build_pycuda/pycuda/src/wrapper/wrap_cudadrv.cpp</cite>:</p>
<div class="highlight-python"><pre>..1 #include &lt;cuda.hpp&gt;
...
.79   class host_alloc_flags { };
...
810 #if CUDAPP_CUDA_VERSION &gt;= 2020
811   {
812     py::class_&lt;host_alloc_flags&gt; cls("host_alloc_flags", py::no_init);
813     cls.attr("PORTABLE") = CU_MEMHOSTALLOC_PORTABLE;
814     cls.attr("DEVICEMAP") = CU_MEMHOSTALLOC_DEVICEMAP;
815     cls.attr("WRITECOMBINED") = CU_MEMHOSTALLOC_WRITECOMBINED;
816   }
817 #endif
818
819 #if CUDAPP_CUDA_VERSION &gt;= 4000
820   {
821     py::class_&lt;mem_host_register_flags&gt; cls("mem_host_register_flags", py::no_init);
822     cls.attr("PORTABLE") = CU_MEMHOSTREGISTER_PORTABLE;
823     cls.attr("DEVICEMAP") = CU_MEMHOSTREGISTER_DEVICEMAP;
824   }
825 #endif</pre>
</div>
</div>
<div class="section" id="memory-pools">
<h2>Memory Pools<a class="headerlink" href="#memory-pools" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://documen.tician.de/pycuda/util.html">http://documen.tician.de/pycuda/util.html</a></li>
</ul>
<p>The functions pycuda.driver.mem_alloc() and pycuda.driver.pagelocked_empty()
can consume a fairly large amount of processing time if they are invoked very
frequently. For example, code based on pycuda.gpuarray.GPUArray can easily run
into this issue because a fresh memory area is allocated for each intermediate
result. Memory pools are a remedy for this problem based on the observation
that often many of the block allocations are of the same sizes as previously
used ones.</p>
<p>Then, instead of fully returning the memory to the system and incurring the
associated reallocation overhead, the pool holds on to the memory and uses it
to satisfy future allocations of similarly-sized blocks. The pool reacts
appropriately to out-of-memory conditions as long as all memory allocations are
made through it. Allocations performed from outside of the pool may run into
spurious out-of-memory conditions due to the pool owning much or all of the
available memory.</p>
</div>
<div class="section" id="ga-to-gpu">
<h2>ga.to_gpu<a class="headerlink" href="#ga-to-gpu" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>creates GPUArray instance which<ul>
<li>allocates device memory</li>
<li>copies htod with  <cite>drv.memcpy_htod(self.gpudata, ary)</cite></li>
</ul>
</li>
</ul>
<p><cite>/usr/local/env/chroma_env/build/build_pycuda/pycuda/pycuda/gpuarray.py</cite>:</p>
<div class="highlight-python"><pre>861 # {{{ creation helpers
862
863 def to_gpu(ary, allocator=drv.mem_alloc):
864     """converts a numpy array to a GPUArray"""
865     result = GPUArray(ary.shape, ary.dtype, allocator, strides=ary.strides)
866     result.set(ary)
867     return result
...
137 class GPUArray(object):
138     """A GPUArray is used to do array-based calculation on the GPU.
139
140     This is mostly supposed to be a numpy-workalike. Operators
141     work on an element-by-element basis, just like numpy.ndarray.
142     """
143
144     __array_priority__ = 100
145
146     def __init__(self, shape, dtype, allocator=drv.mem_alloc,
147             base=None, gpudata=None, strides=None, order="C"):
148         dtype = np.dtype(dtype)
149
...
184         self.allocator = allocator
185         if gpudata is None:
186             if self.size:
187                 self.gpudata = self.allocator(self.size * self.dtype.itemsize)
188             else:
189                 self.gpudata = None
190
191             assert base is None
192         else:
193             self.gpudata = gpudata
194
195         self.base = base
196
197         self._grid, self._block = splay(self.mem_size)
...
204     def set(self, ary):
205         assert ary.size == self.size
206         assert ary.dtype == self.dtype
207         if ary.strides != self.strides:
208             from warnings import warn
209             warn("Setting array from one with different strides/storage order. "
210                     "This will cease to work in 2013.x.",
211                     stacklevel=2)
212
213         assert self.flags.forc
214
215         if self.size:
216             drv.memcpy_htod(self.gpudata, ary)
217</pre>
</div>
</div>
<div class="section" id="to-float3-to-uint3">
<h2>to_float3 to_uint3<a class="headerlink" href="#to-float3-to-uint3" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>copies (N,3) into float32</li>
<li>subclasses to ga.vec.float3</li>
<li>reshape to yield (N)</li>
</ul>
<p><cite>chroma/chroma/gpu/tools.py</cite>:</p>
<div class="highlight-python"><pre>137 def to_float3(arr):
138     "Returns an pycuda.gpuarray.vec.float3 array from an (N,3) array."
139     if not arr.flags['C_CONTIGUOUS']:
140         arr = np.asarray(arr, order='c')
141     return arr.astype(np.float32).view(ga.vec.float3)[:,0]
142
143 def to_uint3(arr):
144     "Returns a pycuda.gpuarray.vec.uint3 array from an (N,3) array."
145     if not arr.flags['C_CONTIGUOUS']:
146         arr = np.asarray(arr, order='c')
147     return arr.astype(np.uint32).view(ga.vec.uint3)[:,0]</pre>
</div>
<div class="highlight-python"><pre>In [1]: a = np.arange(10)

In [2]: a.flags
Out[2]:
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  UPDATEIFCOPY : False

a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)

Copy of the array, cast to a specified type.

a.view(dtype=None, type=None)

New view of array with the same data.

Parameters
----------
dtype : data-type or ndarray sub-class, optional
    Data-type descriptor of the returned view, e.g., float32 or int16. The
    default, None, results in the view having the same data-type as `a`.
    This argument can also be specified as an ndarray sub-class, which
    then specifies the type of the returned object (this is equivalent to
    setting the ``type`` parameter).
type : Python type, optional
    Type of the returned view, e.g., ndarray or matrix.  Again, the
    default None results in type preservation.

In [8]: a = np.arange(30).reshape(10,3)

In [9]: a
Out[9]:
array([[ 0,  1,  2],
       [ 3,  4,  5],
       [ 6,  7,  8],
       [ 9, 10, 11],
       [12, 13, 14],
       [15, 16, 17],
       [18, 19, 20],
       [21, 22, 23],
       [24, 25, 26],
       [27, 28, 29]])

In [10]: a[:,0]
Out[10]: array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])

In [11]: a[:,0].shape
Out[11]: (10,)</pre>
</div>
<div class="sidebar">
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../../geant4/" title="geant4"
             >next</a> |</li>
        <li class="right" >
          <a href="../pycuda_pyopengl_interop/interop_nvidia_examples/" title="NVIDIA Interop Examples"
             >previous</a> |</li>
    <li><a href="/tracs/env/timeline">env</a> &raquo;</li>
    
        <li><a href="../../">Env  documentation</a> &raquo;</li>

          <li><a href="../" >pycuda</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2012, Simon C Blyth.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.
    </div>
  </body>
</html>
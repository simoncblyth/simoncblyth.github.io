<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Simon C Blyth &mdash; Env  documentation</title>
    
    <link rel="stylesheet" href="../../../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/highstock/highstock.js"></script>
    <script type="text/javascript" src="../../../_static/highstock/modules/exporting.js"></script>
    <link rel="top" title="Env  documentation" href="../../../" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
    <li><a href="/tracs/env/timeline">env</a> &raquo;</li>
    
        <li><a href="../../../">Env  documentation</a> &raquo;</li>
 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script><h3>Links</h3>
<ul class="this-page-menu">
	<li><a href="/tracs/env"> env </a>  <a href="/tracs/env/timeline"> tl </a> <a href="/repos/env/trunk"> repo </a> <a href="/e"> edocs </a> </li>
	<li><a href="/tracs/heprez"> heprez </a>  <a href="/tracs/heprez/timeline"> tl </a> <a href="/repos/heprez/trunk"> repo</a> <a href="/h">hdocs</a>   </li>
        <li><a href="/e/scm/monitor/" > backup status </a> </li>
</ul>

<h3>Content Skeleton</h3>

<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">Installing <strong>env</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../base/">Base Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../log/May2012/">LOG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../TODO/">TODO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sysadmin/">Sys Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../plot/">Plotting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../scm/">SCM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../trac/">Trac</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../root/">ROOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sphinxext/">Sphinx Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../matplotlib/">Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nose/">nose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../svn/">SVN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../npy/">Numerical Python, numpy et al</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pypy/">PyPy : faster python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tools/">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mysqlhotcopy/">MySQL hotcopy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mysql/">MySQL Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sqlite/">SQLite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../db/">DB scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../qxml/">QXML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fossil/">Fossil</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../java/">Java Demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda/">cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pycuda/">pycuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../geant4/">geant4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../muon_simulation/">muon_simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../chroma/">chroma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../llvm/">llvm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../graphics/">Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda/">cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../opencl/">opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linux/">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cloud/">Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package_management/">Package Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ui/">ui</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../debugging/">debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mercurial/">mercurial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../javascript/">javascript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nuwa/">nuwa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ccgpu/">ccgpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pygame/">pygame</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../zeromq/">zeromq</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../doc/">doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../python/">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../osx/">osx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hg/">hg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../simoncblyth.bitbucket.org/">simoncblyth.bitbucket.org</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../numpy/">numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../presentation/">Muon Simulation Presentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optix/">optix</a></li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../../_sources/env/report/ntu-simoncblyth-may-2015.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="simon-c-blyth">
<h1>Simon C Blyth<a class="headerlink" href="#simon-c-blyth" title="Permalink to this headline">¶</a></h1>
<div class="section" id="the-optical-photon-simulation-problem">
<h2>The Optical Photon Simulation Problem<a class="headerlink" href="#the-optical-photon-simulation-problem" title="Permalink to this headline">¶</a></h2>
<div class="figure">
<img alt="env/report/env/geant4/geometry/collada/g4daeview/20141224-115935.png" src="env/report/env/geant4/geometry/collada/g4daeview/20141224-115935.png" />
<p class="caption">Side view of simulated Cerenkov photons from a 100 GeV muon travelling from right to left
across the Daya Bay antineutrino detector. The shock front nature is apparent.
Primary particles are simulated by Geant4, Cerenkov &#8220;steps&#8221; of the primaries
are transferred to the GPU and photons are generated, propagated and visualized
all on the GPU. Photons that hit PMTs
are returned back to Geant4 to be included into standard hit collections.
Photon colors indicate reemission (green), absorption(red), specular reflection (magenta), scattering(blue), no history (white).</p>
</div>
<div class="section" id="neutrino-detection-with-optical-photons">
<h3>Neutrino Detection with Optical Photons<a class="headerlink" href="#neutrino-detection-with-optical-photons" title="Permalink to this headline">¶</a></h3>
<p>Incoming neutrinos interact weakly with materials by the inverse beta decay process
where an electron anti-neutrino is captured on a proton of the target resulting
in the production of a positron and a neutron.
The positron yields a prompt energy deposit followed by a delayed
energy deposit from the neutron.
Detector materials and geometry are chosen to enable detection of these energy deposits.
The majority of neutrino detectors operate by the detection of visible light produced
in transparent target materials such as water, ice or liquid scintillator.
The weak nature of the neutrino interactions typically necessitate large volumes
of material and large numbers of Photo Multiplier Tubes (PMT) to detect the light.</p>
</div>
<div class="section" id="importance-of-optical-photons">
<h3>Importance of Optical Photons<a class="headerlink" href="#importance-of-optical-photons" title="Permalink to this headline">¶</a></h3>
<p>Many detectors, including Daya Bay and JUNO, utilize liquid targets with materials
chosen for their scintillation response to energy deposits resulting in re-emission
in the form of visible light. In addition the liquids are required to have sufficient
transparency within the appropriate wavelength range to allow adequate visible light
to reach the surrounding PMTs, which in turn are chosen to have appropriate
quantum efficiency within the wavelength range to convert the incident light
into measurable electrical pulses.
In addition to the dominant source of visible light from scintillation there is
also a significant contribution from Cerenkov radiation.
Cerenkov radiation is emitted when a charged particle travels through a medium
at a speed greater than the speed of light in that medium.</p>
</div>
<div class="section" id="importance-of-monte-carlo-simulation-and-geant4">
<h3>Importance of Monte Carlo Simulation and Geant4<a class="headerlink" href="#importance-of-monte-carlo-simulation-and-geant4" title="Permalink to this headline">¶</a></h3>
<p>High energy physics experiments utilize Monte Carlo simulations to
develop a detailed model of detector operation that provides essential insights
throughout the lifetime of the experiment from detector design, development
of reconstruction algorithms and eventually during data analysis in comparison
with actual data. A detailed quantitative understanding of the entire chain
from particle generation through to optical photon generation, propagation
and detection is necessary for detectors to achieve their design goals and
for data analysis to make best use of the data collected, allowing measurement
uncertainties to be estimated.</p>
<p>The Geant4 simulation toolkit is used as the basis for the simulation
of most High Energy Physics experiments, it has been developed over the past 20 years
principally by Physicists working in Collider experiments, but has been adopted
for the simulation of neutrino detectors including Daya Bay and JUNO and
is used by many Scientists working in other fields including Medical Physics and Space Science.</p>
</div>
<div class="section" id="geant4-optical-photon-simulation">
<h3>Geant4 Optical Photon Simulation<a class="headerlink" href="#geant4-optical-photon-simulation" title="Permalink to this headline">¶</a></h3>
<p>Despite neutrino detectors typically being located underground
to reduce the Cosmic muon background, this usually
remains the principal background. Such cosmic muon
background events in the Daya Bay simulation may yield up to several
millions of optical photons which are individually propagated
through the geometry of the detector by Geant4. Only a small fraction
of the optical photons register as hits on the PMTs.</p>
<p>Unfortunately the serial nature of this propagation and
the large numbers of optical photons make this
the slowest single element of the Daya Bay simulation.
Profiling indicates that more than 95% of CPU time is expended on
propagating optical photons.
The performance of the simulation of other PMT based detectors
is expected to be similarly dominated by the bottleneck of
optical photon simulation.</p>
</div>
<div class="section" id="geant4-geometry-model-inhibits-performant-ray-tracing">
<h3>Geant4 Geometry Model Inhibits Performant Ray Tracing<a class="headerlink" href="#geant4-geometry-model-inhibits-performant-ray-tracing" title="Permalink to this headline">¶</a></h3>
<p>The most computationally intensive aspect of optical photon simulation
is the determination, at each step of the propagation, of the intersection point
of rays representing photon directions with the geometry of the detector, which
is represented in Geant4 by large trees of C++ objects corresponding
to the volumes of the detector.  Other than finding intersections,
all that is computationally required at each step are a handful of
random number draws and lookups of material and surface properties
against simple data structures.</p>
<p>The problem with Geant4 optical photon simulation can be summarised as
poor ray tracing performance caused by a geometry model
which inhibits the use of modern acceleration techniques.</p>
</div>
</div>
<div class="section" id="solving-the-optical-photon-simulation-problem">
<h2>Solving the Optical Photon Simulation Problem<a class="headerlink" href="#solving-the-optical-photon-simulation-problem" title="Permalink to this headline">¶</a></h2>
<div class="section" id="bridging-between-geant4-and-external-simulation">
<h3>Bridging between Geant4 and external simulation<a class="headerlink" href="#bridging-between-geant4-and-external-simulation" title="Permalink to this headline">¶</a></h3>
<p>The Geant4 geometry model is central to the structure
of the simulation toolkit and has adequately served the needs of
Collider experiments for more than 20 years.
Major changes to Geant4 that would only be of value to the minority
who critically depend on optical photon simulation are unlikely.
A hybrid solution combining external optical photon simulation with
standard Geant4 simulation of all other particles is called for.</p>
<p>Fortunately optical photons constitute the final stage of the physical
simulation making it straightforward to integrate the results
of external optical photon generation and propagation back into
Geant4 in the form of collections of PMT hits.
Implementing external optical photon simulation necessitates
bridging geometry, material and surface properties and
PMT identity information from Geant4 to the external simulation.
Also simulated event data needs to be communicated and PMT hits
returned and integrated into Geant4 hit collections.
I developed the G4DAE and G4DAEOpticks software packages
to provide this bridging.</p>
</div>
<div class="section" id="g4dae-bridging-geometry-information">
<h3>G4DAE : Bridging Geometry Information<a class="headerlink" href="#g4dae-bridging-geometry-information" title="Permalink to this headline">¶</a></h3>
<p>My development of the G4DAE geometry exporter began in October 2013,
the exporter writes COLLADA/DAE files (a 3D industry standard format)
containing the Geant4 triangulated vertices of the geometry
together with material and surface properties. The bulk of development
of the G4DAE exporter was completed by early 2014.</p>
<p>The G4DAE exporter has wide applicability within the Geant4 user community as
it allows detector geometries to be visualized with many commercial and open source
packages. Many of these packages use modern OpenGL techniques providing
drastically faster visualization than that provided by Geant4 itself.</p>
<p>I was invited to present the G4DAE package at the 19th Geant4 Collaboration Meeting
held in Okinawa in September 2014. The Geant4 Collaboration accepted my
proposal to contribute the G4DAE exporter to Geant4 and we plan to include it
with the 2015 Geant4 release.  This will entail work to perform the inclusion
into the latest Geant4 version and test the exporter with a
large variety of detector geometries.</p>
</div>
<div class="section" id="g4daeopticks-bridging-event-data">
<h3>G4DAEOpticks : Bridging Event Data<a class="headerlink" href="#g4daeopticks-bridging-event-data" title="Permalink to this headline">¶</a></h3>
<p>G4DAEOpticks provides transport of photons, hits and Cerenkov
and Scintillation generation steps and allows externally formed hits
to be integrated within standard Geant4 hit collections.
I integrated earlier developments to form the G4DAEOpticks runtime bridge
starting from September 2014, the intended features were completed
by January 2015.</p>
<ul class="simple">
<li>request/response network communication of Geant4 data,
implemented using NumPy serialization and ZeroMQ queues,
with optional JSON based metadata</li>
<li>integration of returned PMT hit data into standard Geant4
hit collections allowing subsequent processing such as electronics
simulation to proceed unchanged</li>
<li>data definitions for photons, Cerenkov photons, scintillation photons,
scintillation steps, Cerenkov steps and PMT hits</li>
<li>Geant4 level implementation to facilitate integration with
simulation packages of any experiment via minor changes
to the Scintillation and Cerenkov processes.</li>
</ul>
<p>G4DAEOpticks was originally intended to provide a bridge to external photon propagation
with the Chroma optical photon simulation, however the bridge is general in nature
and can be used unchanged with other external optical photon simulation packages.
To reflect this generality the former name of G4DAEChroma has been changed to G4DAEOpticks.</p>
</div>
<div class="section" id="chroma-gpu-based-optical-photon-simulation">
<h3>Chroma : GPU based Optical Photon Simulation<a class="headerlink" href="#chroma-gpu-based-optical-photon-simulation" title="Permalink to this headline">¶</a></h3>
<p>Chroma is an open source project providing ultra-fast photon simulation using GPU
processing with CUDA. It was originally developed by Stanley Seibert,
University of Pennsylvania, within the context of LBNE detector design studies.
Chroma claims optical photon simulation speeds 200 times faster than Geant4.</p>
<p>The Chroma project provides:</p>
<ul class="simple">
<li>optical photon simulation CUDA C kernels</li>
<li>geometry intersection CUDA C kernels
using a boundary volume heirarchy BVH</li>
<li>infrastructure for compiling and launching kernels using Python, PyCUDA and NumPy</li>
</ul>
<p>Following initial investigations in early 2014, I forked Chroma in April 2014,
my modifications from April 2014 to January 2015 can be summarized:</p>
<ul class="simple">
<li>integrated G4DAE exported geometry loading allowing pre-existing detector geometries
to be uploaded to the GPU and used with Chroma</li>
<li>enabled usage on non-workstation GPUs which demand short kernel launch times to avoid system freezes</li>
<li>adopted CUDA/OpenGL interoperation techniques to allow efficient visualization of Chroma generated data</li>
<li>adopted NumPy serialization format for efficient transport of Geant4 event data, eliminating multiple levels of data marshalling</li>
<li>added generation step transport and optical photon generation via Cerenkov and Scintillation processes,
reducing data transport overheads by a factor of 100 compared to the transport of photons</li>
</ul>
</div>
<div class="section" id="adopting-numpy-serialization-format">
<h3>Adopting NumPy serialization format<a class="headerlink" href="#adopting-numpy-serialization-format" title="Permalink to this headline">¶</a></h3>
<p>In order to transport photon or other data over the network
or between processes it is necessary to first convert objects
into a serialized stream of bytes, transport the bytes and
then deserialize the bytes back into objects at the other end.
The technique initially adopted for communication between
Geant4 and external simulation used ROOT TObject serialization.
This approach was found to be inconvenient due to
the large number of data transformations required
for the communication from Geant4 to external simulation
and onwards to the GPU and back again.</p>
<p>Instead the NumPy serialization format was adopted, the extreme
simplicity of the format allows G4DAEOpticks to effectively fill NumPy arrays directly
from Geant4 C++, which after deserialization can be copied to the
GPU without any transformation.
NumPy is the most popular package for scientific computing with Python.</p>
</div>
<div class="section" id="gpu-generation-of-scintillation-and-cerenkov-photons">
<h3>GPU Generation of Scintillation and Cerenkov Photons<a class="headerlink" href="#gpu-generation-of-scintillation-and-cerenkov-photons" title="Permalink to this headline">¶</a></h3>
<p>Runtime integration by collecting and transporting photons was found to cause large overheads
due to the up to 200MB of photon data per event.
As the primary particles of the simulation step through the materials of the
detector optical photons are regarded to be generated by only two physical processes:</p>
<ul class="simple">
<li>Scintillation light from the scintillating liquids of the detector</li>
<li>Cerenkov light from particles travelling at speeds greater than the
speed of light in the liquids</li>
</ul>
<p>These processes are modelled by G4Cerenkov and G4Scintillation subclasses
in which the number of optical photon secondaries to generate are calculated
followed by a loop over the photons to perform the generation. Typically each step results in
up to 300 optical photons being generated.</p>
<p>Rather than collect photons the approach was modified to collect the parameters
of the Cerenkov and Scintillation steps including the number of photons to generate.
Using these step parameters as inputs I reimplemented the optical
photon generation in CUDA C as an extension to Chroma, allowing GPU generation
of the optical photons. This modification allowed the amount of transported data per simulated event
to be reduced by a factor of 100. Also the requirement to copy the photon data
to the GPU was avoided.
Near perfect agreement between distributions of the parameters of the GPU generated photons
and Geant4 counterparts has been achieved.</p>
</div>
</div>
<div class="section" id="background-on-optix-and-migration-progress">
<h2>Background on OptiX and Migration Progress<a class="headerlink" href="#background-on-optix-and-migration-progress" title="Permalink to this headline">¶</a></h2>
<div class="section" id="finding-nvidia-optix">
<h3>Finding NVIDIA OptiX<a class="headerlink" href="#finding-nvidia-optix" title="Permalink to this headline">¶</a></h3>
<p>Chroma&#8217;s python based infrastructure provides an excellent fast development
and learning environment, however for production usage a C++ infrastructure
is preferable as this facilitates library integration with other projects
and makes multi-threading easier. Ease of multi-threading is particularly important
as it is required to make efficient use of multiple GPUs.</p>
<p>Towards the end of 2014, I became increasingly concerned regarding
the difficult development work that would be necessary to allow
Chroma to make efficient use of multiple GPUs.
My searches for projects making efficient use of multiple GPUs
led me to NVIDIA OptiX.  OptiX is a C++/CUDA based framework
providing accelerated ray tracing. The OptiX project
makes compelling claims:</p>
<ul class="simple">
<li>state-of-the-art GPU accelerated ray tracing performance,
improved with each release of OptiX and tuned to fully exploit
new GPU architectures</li>
<li>performance scaling across multiple GPUs, or even across multiple networked
machines, with little development effort</li>
<li>support for analytic geometry definition avoiding the need to tesselate
and instanced geometry avoiding repetition of data;
potentially drastic memory and performance savings may be achievable
by exploiting these features to describe the repeated PMT geometry</li>
</ul>
<p>Clearly shifting the burden of handling acceleration techniques
and efficient GPU/multi-GPU utilization to OptiX, a project actively
maintained by NVIDIA engineers, is hugely advantageous compared to
using the ray tracing acceleration provided by the Chroma project.</p>
<p>The OptiX framework is freely distributed by NVIDIA for non-commercial usage.
It is used as the basis for NVIDIA&#8217;s commercial renderers: Iray and mental ray,
suggesting that OptiX will remain actively maintained and improved over the coming years.
These commercial renderers use OptiX to provide photorealistic image rendering
using physically based material definitions to many companies in
the Design and Film industries, including Adobe, Canon, Sony, Honda, Lockheed,
Pixar and Disney.</p>
</div>
<div class="section" id="optix-raycasting">
<h3>OptiX Raycasting<a class="headerlink" href="#optix-raycasting" title="Permalink to this headline">¶</a></h3>
<div class="figure">
<img alt="env/report/env/optix/raycast/optix001.png" src="env/report/env/optix/raycast/optix001.png" />
<p class="caption">NVIDIA OptiX raycast of Daya Bay near site geometry at interactive speeds
of 30 frames per second for 1024x768 pixels, corresponding to 23M
intersection tests per second on a MacBook Pro (2013) with NVIDIA GeForce GT 750M 2048 MB.</p>
</div>
<p>With OptiX raycast renders can be generated at interactive
speeds approaching 30 frames per second. This contrasts
markedly with Chroma where comparable renders take ~1.8s per frame,
corresponding to a 50x speedup.
This speedup factor is sufficiently large to make
development of carefully fair comparisons unnecessary.</p>
</div>
<div class="section" id="optix-multiple-gpu-scaling">
<h3>OptiX Multiple GPU Scaling<a class="headerlink" href="#optix-multiple-gpu-scaling" title="Permalink to this headline">¶</a></h3>
<p>In order to assess OptiX scaling across GPU cores
the performance of a borrowed workstation at
IHEP with two Tesla K20m GPUs was compared with
my laptop via rendering benchmarks with the Daya Bay
geometry. The same code was run on my laptop and
on the workstation in two different configurations.</p>
<ul class="simple">
<li>2 Tesla K20m (4992 cores)  28.0 ms/f</li>
<li>1 Tesla K20m (2496 cores)  49.1 ms/f</li>
<li>1 GeForce GT 750m  (382 cores) 345.1 ms/f</li>
</ul>
<p>Raycast rendering performance was found to be approximately linear
with the number of CUDA cores, with the 2 GPU workstation which has 13x
the number of cores being 12x faster than my laptop. This indicates that
OptiX is succeeding to schedule and balance work to the GPUs
in a near optimal manner.</p>
</div>
<div class="section" id="migration-of-infrastructure-to-c">
<h3>Migration of Infrastructure to C++<a class="headerlink" href="#migration-of-infrastructure-to-c" title="Permalink to this headline">¶</a></h3>
<p>The OptiX framework provides host and device side APIs in C++ and CUDA C.
Although complicated approaches to retain some of the existing python
infrastructure could have been attempted, my experience directed
me to the simplest possible approach of reimplementing the infrastructure
using equivalent C++ libraries to the python dependencies where available
as being the best approach to minimise code maintenance over the longterm.</p>
<p>External optical photon simulation depends on the infrastructure
both in operation and for its development and debugging.
Aspects covered include the below, some of these are detailed
in subsequent sections.</p>
<ul class="simple">
<li>conversion of geometry and material/surface properties into the OptiX model</li>
<li>OpenGL rasterized visualization of geometry and event data</li>
<li>OptiX raytraced visualization of geometry data</li>
<li>cuRAND random number generation operational within OptiX program</li>
<li>wavelength interpolated material/surface property lookups</li>
</ul>
<p>Application steering, asynchronous IO and event infrastructure
have been reimplemented using the Asio-ZMQ package and
several Boost C++ libraries including Boost.Asio, Boost.Program_options
and Boost.Filesystem replacing python standard library packages and glumpy.</p>
<p>This migration work was done from Feb-April 2015 and is ongoing.</p>
</div>
<div class="section" id="optix-geometry-handling-using-assimp">
<h3>OptiX Geometry Handling using Assimp<a class="headerlink" href="#optix-geometry-handling-using-assimp" title="Permalink to this headline">¶</a></h3>
<p>Geometry and material and surface properties parsing of
G4DAE exported geometry files have been reimplemented using
my fork of the Assimp project replacing PyCOLLADA.</p>
<p>Assimp is an open source 3D geometry importer that supports a large number
of file formats including DAE/COLLADA. I forked Assimp on github in February 2015
in order to add support for the extra material and surface properties included
with G4DAE geometry exports from Geant4.</p>
<p>The complexity and size of the Assimp package made me reluctant
for it to become a permanent dependency. Due to this I created the
GGeo geometry package to act as an intermediary geometry format.
My small AssimpWrap package orchestrates creation of the GGeo model
from the imported Assimp model. The OptiX model is then created entirely
from the GGeo model.
GGeo provides a simple environment, that I fully control,
in which to experiment with alternative approaches to creating OptiX geometries
and representing surface and material properties within OptiX.
Using GGeo I have developed a way of packing tables of material and surface
properties as a function of wavelength into GPU textures.
Wavelength dependent property lookups can then benefit from GPU hardware interpolation.</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/simoncblyth/assimp">https://github.com/simoncblyth/assimp</a></li>
<li><a class="reference external" href="https://bitbucket.org/simoncblyth/env/src/tip/optix/ggeo">https://bitbucket.org/simoncblyth/env/src/tip/optix/ggeo</a></li>
</ul>
</div>
<div class="section" id="migration-of-visualization">
<h3>Migration of Visualization<a class="headerlink" href="#migration-of-visualization" title="Permalink to this headline">¶</a></h3>
<p>The prior visualizations of geometry and event data were based on PyOpenGL
which forced the use of an ancient OpenGL version 2.1.
The migration to C++ infrastructure has allowed adoption of modern OpenGL 4.1
which opens new possibilites for efficient visualization of OptiX generated
data by using OptiX/OpenGL interoperation features.</p>
<p>OGLRap is a package of classes I developed
to simplify the use of modern OpenGL, for example calculating
the 4x4 homogenous matrices needed to control 3D view points
and projections. Also renderer classes that simplify the
use of OpenGL shaders to visualize geometry and event data are included.</p>
<ul class="simple">
<li><a class="reference external" href="https://bitbucket.org/simoncblyth/env/src/tip/graphics/oglrap">https://bitbucket.org/simoncblyth/env/src/tip/graphics/oglrap</a></li>
</ul>
</div>
<div class="section" id="optix-architecture">
<h3>OptiX Architecture<a class="headerlink" href="#optix-architecture" title="Permalink to this headline">¶</a></h3>
<p>The OptiX engine provides a programmable ray tracing pipeline
analogous to the OpenGL rasterization pipeline where
user supplied OptiX programs take the place of OpenGL shaders.
OptiX however focuses exclusively on the fundamental ray tracing computations
and avoids making rendering specific assumptions.
Seven different user supplied program types are compiled
into a single optimized CUDA kernel. NVIDIA expertise regarding
specific GPU architectures and ray tracing workloads is used
in the optimization.</p>
<ul class="simple">
<li>Ray Generation programs provides the entry point into the ray tracing pipeline,
they start the trace and store results into output buffers.</li>
<li>Intersection programs implement ray-geometry intersection tests which are
invoked to perform a geometric queries as acceleration structures are traversed.
Simple ray triangle intersection could be provided but also
analytic geometry intersection is possible.</li>
<li>Bounding box programs compute the bounds associated with each primitive to
enable acceleration structures over arbitrary geometry</li>
<li>Closest hit programs are invoked once traversal has found the closest
intersection of a ray with the scene geometry. They can cast new rays
and store results into the ray payload.</li>
<li>Any Hit programs are called during traversal for every ray-object
intersection, the default of no operation is often appropriate.</li>
<li>Miss programs are executed when the ray does not intersect any geometry</li>
<li>Exception programs are called when problems such as stack overflow occur</li>
</ul>
<p>The bulk of initial development effort for the optical photon simulation is expected to
be within the ray generation and closest hit programs.  In addition subsequent
developments to adopt more memory efficient representations of repeated PMT geometry
would require work on the intersection and bounding box programs.</p>
</div>
<div class="section" id="random-number-generation-in-optix-programs">
<h3>Random Number Generation in OptiX programs<a class="headerlink" href="#random-number-generation-in-optix-programs" title="Permalink to this headline">¶</a></h3>
<p>Chroma uses the cuRAND library, which is part of the CUDA toolkit,
for the concurrent generation of millions of reproducible sequences
of pseudorandom numbers. Concurrent generation is handled by assigning
sub-sequences to each planned CUDA thread, and having the threads maintain
state regarding positions within their sub-sequence.
The cuRAND library requires initialization of these per-thread states with a CUDA kernel launch.</p>
<p>Initial attempts to initialize cuRAND state within OptiX programs failed,
only by increasing the OptiX stack size by a factor of 10 could initialization be made to succeed.
However with such large stack size the OptiX kernel was found to run very slowly.</p>
<p>A workaround was found to avoid this impasse by using a separate sequence of
CUDA kernel launches to initialize the cuRAND state, persist that state to a cache file,
and then load the state from the file into OptiX using OptiX/CUDA interop techniques.
This allows OptiX to start from the initialized cuRAND state without needing
the large stack size. I packaged this work into CUDAWrap.</p>
<ul class="simple">
<li><a class="reference external" href="https://bitbucket.org/simoncblyth/env/src/tip/cuda/cudawrap">https://bitbucket.org/simoncblyth/env/src/tip/cuda/cudawrap</a></li>
</ul>
</div>
</div>
<div class="section" id="plans-for-the-year-ahead">
<h2>Plans for the year ahead<a class="headerlink" href="#plans-for-the-year-ahead" title="Permalink to this headline">¶</a></h2>
<div class="section" id="porting-photon-generation-and-propagation-to-optix">
<h3>Porting Photon Generation and Propagation to OptiX<a class="headerlink" href="#porting-photon-generation-and-propagation-to-optix" title="Permalink to this headline">¶</a></h3>
<p>Cerenkov and Scintillation photon generation based on buffers
of generation step parameters clearly belong within the
OptiX ray generation program.  From experience with Chroma
porting the generation to OptiX is expected to be straightforward,
however it constitutes the first real test of the random number generation
approach adopted and may necessitate further infrastructure development.</p>
<p>Bringing Chroma photon simulation kernels into the OptiX model
will entail dividing the Chroma propagation stepping loop
between the OptiX ray generation and closest hit programs.
As these stages communicate via a ray payload structure there is
some flexibility and some experimentation is required to find
a workable solution.
The last step of forming GPU hits requires further infrastructure
work to bring PMT identity information into OptiX.</p>
</div>
<div class="section" id="validation-of-photon-generation-and-propagation-within-optix">
<h3>Validation of Photon Generation and Propagation within OptiX<a class="headerlink" href="#validation-of-photon-generation-and-propagation-within-optix" title="Permalink to this headline">¶</a></h3>
<p>The photon generation code has already been validated by comparison against Geant4
within the Chroma context, thus it is expected to be straightforward to verify
again within OptiX. Validation of the propagation is much less certain,
it is expected that both the Geant4 and OptiX simulation will need to be
instrumented to allow detailed step by step comparisons to be performed</p>
</div>
<div class="section" id="opengl-visualization-of-optix-photon-propagation">
<h3>OpenGL Visualization of OptiX Photon Propagation<a class="headerlink" href="#opengl-visualization-of-optix-photon-propagation" title="Permalink to this headline">¶</a></h3>
<p>Development and debugging with Chroma was greatly facilitated
by 3D visualizations of geometries and event data.
GPU performance and the capabilities of CUDA, OpenGL and OptiX
to share access to GPU resident data structures has
enabled unprecedented within HEP visualizations.
I plan to port features from the former python based visualization
into the C++ infrastructure as they are needed for simulation
development.</p>
</div>
<div class="section" id="integration-of-g4dae-geometry-exporter-with-2015-release-of-geant4">
<h3>Integration of G4DAE Geometry Exporter with 2015 release of Geant4<a class="headerlink" href="#integration-of-g4dae-geometry-exporter-with-2015-release-of-geant4" title="Permalink to this headline">¶</a></h3>
<p>At the 19th Geant4 Collaboration Meeting in September 2014,
the Geant4 Collaboration accepted my proposal to contribute the G4DAE exporter.
The work of including it ready for release at the end of 2015 remains to be done.
It is unclear how much work this will entail as the exporter has so far
only been applied to a limited number of detector geometries.</p>
</div>
<div class="section" id="adopt-more-memory-efficient-geometry-representation">
<h3>Adopt more memory efficient Geometry representation<a class="headerlink" href="#adopt-more-memory-efficient-geometry-representation" title="Permalink to this headline">¶</a></h3>
<p>Neutrino detectors, such as the planned JUNO detector, often
have very large numbers of PMTs.
The geometry representation currently in use simply repeats
the tesselated PMT geometry for every PMT.
This initial approach has the advantage of simplicity by the
disadvantage of large GPU memory requirements.</p>
<p>GPU performance is typically constrained by memory access time
thus adopting a geometry representation with much smaller memory demands
will not only allow use of GPUs with less available memory but has
the potential to yield significant performance improvements.
OptiX has two features that may allow a much more efficient
geometry representation to be used:</p>
<ul class="simple">
<li>on GPU instanced geometry, allowing repeated geometry to
be represented without duplication by using associated
transformation matrices which are applied to photon paths
rather than geometry.</li>
<li>GPU algorithmic geometry definition potentially avoids the
need to tesselate altogether, instead a small number of parameters
that describe the PMT shape could be used together with an
algorithm that computes ray intersections. This is similar to
the way Geant4 operates serially on the CPU.</li>
</ul>
</div>
<div class="section" id="moving-into-production">
<h3>Moving Into Production<a class="headerlink" href="#moving-into-production" title="Permalink to this headline">¶</a></h3>
<p>For fast development it is convenient for Geant4 processing
and Optical Photon Simulation to run on different machines
connected via my G4DAEOpticks message queue based infrastructure.
Although such a split architecture remains an option in production
it is possible that it will be advantageous to locate these together
on the same machine or even within the same process.</p>
<p>The ZeroMQ transport mechanism that G4DAEOpticks is based upon
promises to allow simple transitions between such different configurations.
Some experimentation is neverthless required to find a workable solution
within the memory constraints of the available workstation.
The migration to a C++ infrastructure together with use of Boost threading
and asynchronous IO libraries makes the likely need for multi-threaded
architectures much more straightforward to implement.</p>
<p>The JUNO IHEP group has purchased a 4 GPU workstation that hopefully
can be used to perform these configuration experiments
and make performance tests.
Production runs creating large Monte Carlo samples
for validation and use by analysis groups
will require working closely with the Monte Carlo Production
groups of the Daya Bay and JUNO Collaborations.</p>
</div>
<div class="section" id="capitalizing-on-the-infrastructure-developed">
<h3>Capitalizing on the Infrastructure developed<a class="headerlink" href="#capitalizing-on-the-infrastructure-developed" title="Permalink to this headline">¶</a></h3>
<p>A side benefit of this work has been high performance
interactive 3D visualizations of detector geometries and events.
The machinery developed to liberate the geometry data and
allow external simulation can directly be repurposed to implement
an event display framework which to first order could be
used by any experiment that uses Geant4.</p>
<p>I expect this would be of particular interest to smaller Collaborations
lacking the manpower to develop a high performance event display.
Creating a shared software repository in which a framework for
building event displays can be developed would eliminate
duplication of effort.
Development of Daya Bay and JUNO event displays on
top of this generic framework would be a good way
to initiate this shared framework.</p>
<p>Such a repository and the framework would also provide a
channel for demonstrating the use of GPU based optical photon simulation.
Although a layered approach would need to be implemented as OptiX and CUDA
require NVIDIA GPUs whereas OpenGL visualization is usable on almost any machine.</p>
</div>
<div class="section" id="distribution-and-marketing">
<h3>Distribution and Marketing<a class="headerlink" href="#distribution-and-marketing" title="Permalink to this headline">¶</a></h3>
<p>Once validations are well advanced and impressive performance measurements
have been achieved it will be appropriate to market the approach and implementation
with the aim of making GPU accelerated optical photon simulation into a
widely adopted technique in PMT based experiments.</p>
<p>The machinery needs to be made straightforward to obtain, install, integrate and use.
To this end a dedicated software repository website
to manage the code and documentation needs to be created
to act as focal point for communication.</p>
<p>Publicity channels such as collaboration talks, conference talks,
technical papers and video sharing sites need to be used to bring
the technique to a wider audience.</p>
</div>
</div>
<div class="section" id="status-summary">
<h2>Status Summary<a class="headerlink" href="#status-summary" title="Permalink to this headline">¶</a></h2>
<div class="section" id="development-overview">
<h3>Development Overview<a class="headerlink" href="#development-overview" title="Permalink to this headline">¶</a></h3>
<p>The design adopted to transparently replace the
Geant4 simulation of optical photons with an external
GPU based simulation is comprised of geometry, runtime and propagation categories.</p>
<p>Recreation of Geant4 geometry information on the GPU (G4DAE)
was implemented between October 2013 and January 2014, this includes:</p>
<ul class="simple">
<li>Export Geant4 geometry as COLLADA/DAE files</li>
<li>Import geometry files into external simulation and upload to GPU</li>
</ul>
<p>Runtime bridging between Geant4 and external simulation (G4DAEOpticks)
was implemented between September 2014 and January 2015, comprising:</p>
<ul class="simple">
<li>collect photon data or generation step parameters within
the Cerenkov and Scintillation implementations and prevent
further photon processing by Geant4</li>
<li>send data to external process and receive reply containing PMT hits</li>
<li>integrate hits into normal Geant4 hit collections</li>
</ul>
<p>GPU optical photon simulation is currently being transitioned from Chroma to OptiX.
The infrastructure was prepared between January-April 2015, and
the porting to OptiX is in progress from April 2015. Broad aspects:</p>
<ul class="simple">
<li>receive requests over the network</li>
<li>deserialize bytes received into NumPy arrays and copy these to the GPU</li>
<li>generate photons and propagate them through the geometry and form hits</li>
<li>copy photons back from the GPU and reply to Geant4 with hits</li>
</ul>
</div>
<div class="section" id="x-performance-at-a-price">
<h3>1000x performance, at a price<a class="headerlink" href="#x-performance-at-a-price" title="Permalink to this headline">¶</a></h3>
<p>Large performance factors make good headlines, but the reality
is that beyond a certain level of perhaps 100x the optical photon
processing time becomes effectively zero compared to other processing.
Given the raw intersection speed of OptiX and its scaling performance
across multiple GPUs I am confident that performance will
reach such a level. Using external GPU optical photon simulation
will just become the way things are done.</p>
<p>The latest OptiX version 3.8 supports connection to one or more
remote Visual Computing Appliances (VCAs). VCAs are dedicated GPU machines
housing 8 NVIDIA Maxwell GPUs. OptiX based applications can with
minimal changes use the compute power of clusters of such VCAs.
However more modest GPU workstations are expected to provide sufficient
performance to make optical photon processing no longer
the rate determining step of simulation production.</p>
</div>
<div class="section" id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h3>
<p>I have performed a major re-implementation of simulation codes
over the past four months, moving to a C++ infrastructure
in order to benefit from NVIDIA OptiX.  Over the past year, I completed
development of the G4DAEOpticks bridge enabling Geant4 simulations
to transparently benefit for external optical photon simulation.
The ongoing transition to OptiX has yielded immediate performance benefits,
but more importantly the future progression with OptiX releases
and GPU architectures looks to be assured.  Also the
scaling across multiple GPUs and even multiple machines
means achieving a performance level that makes the
optical photon processing time effectively zero is highly probable.</p>
<p>Migration to OptiX has delayed the validation of external simulation
but has also brought this work much closer to being suitable for
production usage as delays from the development of efficient
multi-GPU operation have been avoided.</p>
<p>Beyond the initial porting and validation the major focus
of the year ahead is bringing this work into production usage first within
the Daya Bay and JUNO Collaborations and then to the wider community
of PMT based experiments.</p>
</div>
<div class="section" id="other-work-daya-bay-infrastructure">
<h3>Other Work : Daya Bay Infrastructure<a class="headerlink" href="#other-work-daya-bay-infrastructure" title="Permalink to this headline">¶</a></h3>
<p>Most of the software infrastructure systems I have developed or integrated over the past years
remain in continuous operation with multiple processes running around the clock
at the main Daya Bay institutions, monitoring every database update
and every change to the Daya Bay software, performing software builds and tests
and notifying experts of failure conditions.</p>
<p>My responsibilities include:</p>
<ul class="simple">
<li>Software Infrastructure (Trac and SVN servers)</li>
<li>Offline Database (Management, Operating procedures, software interfaces)</li>
<li>Online Slow Control Database (Monitoring and &#8220;Scraping&#8221; from Online to Offline)</li>
<li>Data Quality Database backup</li>
<li>System Administration (backups)</li>
</ul>
<p>Despite the migration of some aspects of this support work to colleagues
I remain the expert when problems or non standard situations arise.
Occasional urgent requests for help with development of custom scripts,
or with the usage of tools I developed are the most time consuming aspects of these responsibilities.</p>
<div class="sidebar">
date : May 1, 2015 title : Optical Photon Simulation on GPUs </div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
    <li><a href="/tracs/env/timeline">env</a> &raquo;</li>
    
        <li><a href="../../../">Env  documentation</a> &raquo;</li>
 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2012, Simon C Blyth.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.
    </div>
  </body>
</html>